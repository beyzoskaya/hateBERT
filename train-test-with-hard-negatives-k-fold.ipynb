{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df = df.drop(columns=['annotaters'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    df_clean['hate'] = df_clean['label'].map({'h': 1, 'nh': 0}) # binary hate, non-hate\n",
    "    \n",
    "    target_map = {'p': 0, 'e': 1, 'r': 2} # numeric mapping of target categories\n",
    "    df_clean['target'] = df_clean['target'].str.lower().str.strip()\n",
    "    \n",
    "    # non-hate labels have no target\n",
    "    df_clean['target'] = (\n",
    "        df_clean['target']\n",
    "        .map(target_map)\n",
    "        .where(df_clean['target'].isin(target_map.keys()))\n",
    "    )\n",
    "    df_clean['target'] = df_clean['target'].fillna(-100).astype(int)\n",
    "    \n",
    "    invalid_hate_mask = (df_clean['hate'] == 1) & (df_clean['target'] == -100)\n",
    "    df_clean.loc[invalid_hate_mask, 'hate'] = 0\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/turkish-bert/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "class TurkishHateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, hate_labels, target_labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.hate_labels = hate_labels\n",
    "        self.target_labels = target_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'hate_labels': torch.tensor(self.hate_labels[idx], dtype=torch.float),\n",
    "            'target_labels': torch.tensor(self.target_labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurkishHateBERT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "        self.hate_head = torch.nn.Linear(768, 1)\n",
    "        self.target_head = torch.nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.hate_head(pooled_output), self.target_head(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import random\n",
    "\n",
    "turkish_augmenter = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-multilingual-cased',\n",
    "    action=\"substitute\",\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sentences(df, augmenter, aug_p=0.3, max_aug_per_sample=1):\n",
    "    augmented_rows = []\n",
    "    printed = 0\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        if row['hate'] == 1 and random.random() < aug_p:\n",
    "            try:\n",
    "                for _ in range(max_aug_per_sample):\n",
    "                    aug_text = augmenter.augment(row['text'])\n",
    "                    if isinstance(aug_text, list):\n",
    "                        aug_text = aug_text[0]\n",
    "                    new_row = row.copy()\n",
    "                    new_row['text'] = aug_text\n",
    "                    augmented_rows.append(new_row)\n",
    "\n",
    "                    if print_examples and printed < 10:  \n",
    "                        print(f\"Original : {row['text']}\")\n",
    "                        print(f\"Augmented: {aug_text}\")\n",
    "                        print(\"-\" * 60)\n",
    "                        printed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Augmentation failed for row {i}: {e}\")\n",
    "                continue\n",
    "    if augmented_rows:\n",
    "        aug_df = pd.DataFrame(augmented_rows)\n",
    "        print(f\"Added {len(aug_df)} augmented samples.\")\n",
    "        return pd.concat([df, aug_df], ignore_index=True)\n",
    "    else:\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders_augmented(train_df, val_df, tokenizer, batch_size=16):\n",
    "    train_dataset = TurkishHateSpeechDataset(\n",
    "        train_df['text'].values,\n",
    "        train_df['hate'].values,\n",
    "        train_df['target'].values,\n",
    "        tokenizer\n",
    "    )\n",
    "    val_dataset = TurkishHateSpeechDataset(\n",
    "        val_df['text'].values,\n",
    "        val_df['hate'].values,\n",
    "        val_df['target'].values,\n",
    "        tokenizer\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hard_negatives(model, df_train, tokenizer, device, threshold=0.3, max_add=100):\n",
    "    model.eval()\n",
    "    non_hate_df = df_train[df_train['hate'] == 0].copy()\n",
    "    texts = non_hate_df['text'].tolist()\n",
    "    hate_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), 32):\n",
    "            batch_texts = texts[i:i+32]\n",
    "            encodings = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            hate_logits, _ = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(hate_logits).detach().cpu().numpy().flatten()\n",
    "            hate_preds.extend(probs)\n",
    "    non_hate_df['hate_prob'] = hate_preds\n",
    "    hard_negatives = non_hate_df[non_hate_df['hate_prob'] > threshold].copy()\n",
    "    hard_negatives = hard_negatives.sample(min(len(hard_negatives), max_add), random_state=42)\n",
    "    print(f\"Adding {len(hard_negatives)} hard negatives to training data\")\n",
    "    df_new_train = pd.concat([df_train, hard_negatives], ignore_index=True)\n",
    "    return df_new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    hate_preds = []\n",
    "    hate_probs = []\n",
    "    true_hate = []\n",
    "    target_preds = []\n",
    "    true_target = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            hate_logits, target_logits = model(input_ids, attention_mask)\n",
    "            batch_probs = torch.sigmoid(hate_logits.squeeze()).cpu().numpy()\n",
    "            batch_preds = (batch_probs > 0.5).astype(int)\n",
    "            hate_probs.extend(batch_probs)\n",
    "            hate_preds.extend(batch_preds)\n",
    "            true_hate.extend(batch['hate_labels'].cpu().numpy())\n",
    "            target_probs = torch.softmax(target_logits, dim=1).cpu().numpy()\n",
    "            batch_target_preds = np.argmax(target_probs, axis=1)\n",
    "            target_preds.extend(batch_target_preds)\n",
    "            true_target.extend(batch['target_labels'].cpu().numpy())\n",
    "    target_mask = np.array(true_target) != -100\n",
    "    filtered_target_pred = np.array(target_preds)[target_mask]\n",
    "    filtered_true_target = np.array(true_target)[target_mask]\n",
    "    return {\n",
    "        'true_hate': true_hate,\n",
    "        'pred_hate': hate_preds,\n",
    "        'true_target': filtered_true_target,\n",
    "        'pred_target': filtered_target_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_hard_neg(model, train_df, val_df, tokenizer, device, epochs=4, batch_size=16, lr=2e-5, threshold=0.3):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    hate_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    target_criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        train_loader, val_loader = prepare_loaders_augmented(train_df, val_df, tokenizer, batch_size)\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            hate_labels = batch['hate_labels'].to(device)\n",
    "            target_labels = batch['target_labels'].to(device)\n",
    "            hate_logits, target_logits = model(input_ids, attention_mask)\n",
    "            hate_loss = hate_criterion(hate_logits.squeeze(), hate_labels)\n",
    "            target_mask = (target_labels != -100)\n",
    "            if target_mask.any():\n",
    "                target_loss = target_criterion(target_logits[target_mask], target_labels[target_mask])\n",
    "            else:\n",
    "                target_loss = torch.tensor(0.0).to(device)\n",
    "            loss = hate_loss + target_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train loss: {total_loss/len(train_loader):.4f}\")\n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        print(classification_report(val_metrics['true_hate'], val_metrics['pred_hate'], target_names=['Non-Hate', 'Hate']))\n",
    "        print(\"\\nTarget Classification (Hate Cases Only):\")\n",
    "        print(classification_report(val_metrics['true_target'], val_metrics['pred_target'], target_names=['Politics', 'Ethnicity', 'Religion']))\n",
    "        # Add hard negatives for next epoch\n",
    "        train_df = add_hard_negatives(model, train_df, tokenizer, device, threshold=threshold)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df_clean, df_clean['hate'])):\n",
    "    print(f\"\\n\\n=== Fold {fold+1} ===\")\n",
    "    \n",
    "    df_train_fold = df_clean.iloc[train_idx].reset_index(drop=True)\n",
    "    df_val_fold = df_clean.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    print(\"Augmenting training fold...\")\n",
    "    df_train_augmented = augment_sentences(df_train_fold, turkish_augmenter, aug_p=0.3, max_aug_per_sample=1)\n",
    "    \n",
    "    model_fold = TurkishHateBERT()\n",
    "    \n",
    "    print(\"Training model with hard negatives...\")\n",
    "    trained_model = train_model_with_hard_neg(\n",
    "        model_fold,\n",
    "        df_train_augmented,\n",
    "        df_val_fold,\n",
    "        tokenizer,\n",
    "        device,\n",
    "        epochs=4,\n",
    "        batch_size=16,\n",
    "        lr=2e-5,\n",
    "        threshold=0.3\n",
    "    )\n",
    "    \n",
    "    val_loader = prepare_loaders_augmented(df_val_fold, df_val_fold, tokenizer, batch_size=16)[1]\n",
    "    metrics = evaluate_model(trained_model, val_loader, device)\n",
    "    fold_metrics.append(metrics)\n",
    "    \n",
    "    print(f\"\\nFold {fold+1} Classification Report (Validation):\")\n",
    "    print(classification_report(metrics['true_hate'], metrics['pred_hate'], target_names=['Non-Hate', 'Hate']))\n",
    "    print(classification_report(metrics['true_target'], metrics['pred_target'], target_names=['Politics', 'Ethnicity', 'Religion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "hate_precisions, hate_recalls, hate_f1s = [], [], []\n",
    "target_precisions, target_recalls, target_f1s = [], [], []\n",
    "\n",
    "for metrics in fold_metrics:\n",
    "    # Hate speech metrics\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        metrics['true_hate'], metrics['pred_hate'], average='binary', pos_label=1\n",
    "    )\n",
    "    hate_precisions.append(p)\n",
    "    hate_recalls.append(r)\n",
    "    hate_f1s.append(f1)\n",
    "\n",
    "    # Target classification metrics (only on valid target labels)\n",
    "    if len(metrics['true_target']) > 0:\n",
    "        p_t, r_t, f1_t, _ = precision_recall_fscore_support(\n",
    "            metrics['true_target'], metrics['pred_target'], average='macro', zero_division=0\n",
    "        )\n",
    "        target_precisions.append(p_t)\n",
    "        target_recalls.append(r_t)\n",
    "        target_f1s.append(f1_t)\n",
    "\n",
    "# average hate speech metrics\n",
    "avg_hate_precision = np.mean(hate_precisions)\n",
    "avg_hate_recall = np.mean(hate_recalls)\n",
    "avg_hate_f1 = np.mean(hate_f1s)\n",
    "\n",
    "# average target classification metrics\n",
    "avg_target_precision = np.mean(target_precisions) if target_precisions else float('nan')\n",
    "avg_target_recall = np.mean(target_recalls) if target_recalls else float('nan')\n",
    "avg_target_f1 = np.mean(target_f1s) if target_f1s else float('nan')\n",
    "\n",
    "print(\"\\n=== Average metrics across folds ===\")\n",
    "print(f\"Hate Speech Detection - Precision: {avg_hate_precision:.4f}, Recall: {avg_hate_recall:.4f}, F1: {avg_hate_f1:.4f}\")\n",
    "print(f\"Target Classification (hate cases) - Precision: {avg_target_precision:.4f}, Recall: {avg_target_recall:.4f}, F1: {avg_target_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turkish-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
